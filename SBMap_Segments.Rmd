---
title: "SBMap_Segments"
author: "A.Bock"
date: "September 30, 2016"
output: pdf_document
---
This document is a summary of work and code being developed for the CDI project "Automated Mapping of SB OBjects"

This script will acess SB data from a given ScienceBase ID.  We are using Kathy's PRMS data for this application.
Functions are a part of package specificed (*sbtools::item_get*  *item_get* is function in *sbtools* library)

## 1.  Accessing ScienceBase Items

This portion of the script will access the SB item and its content fiven a SB item id (the last part of the 
ScienceBase item page URL).

```{r}
# functions are a part of package specificed (sbtools::item_get - item_get is function in sbtools library)

# get the SB item information
# kathy's data on my page - 57114f7be4b0ef3b7ca554e8
# This is the SB id for the CDI Project - 56f419c5e4b0f59b85e0bc5d
# This is the SB id for Kathy's PRMS Project - 5522bdf7e4b027f0aee3d039
# This is the child SB id for Kathy's Streamflow data - 55b69c6ae4b09a3b01b5f653
PRMS_item<-sbtools::item_get("5522bdf7e4b027f0aee3d039")
names(PRMS_item)
```

Note that the streamflow data is a "child item" of Kathy's page (Appendix 2), so we can use the 
upstream navigation downstream to get at that information.

```{r}
# The PRMS streamflow data is a child ("Appendix 2") of Kathy's data
children<-sbtools::item_list_children(PRMS_item)
# We need the ScienceBase Id of Kathy's streamflow da"ta, 
# To do this we extract the "tail" of the URL of the child item
sFlow_SBid<-tail(unlist(strsplit(children[[2]]$link$url,"/")),1)
#Create object for the SFlow data
child_item<-sbtools::item_get(sFlow_SBid)
print(sbtools::item_list_files(child_item)$fname[1:10])
```

## 2. Retrieving Spatial Components

This portion of the code retrieves the spatial components of the project (stream segments and
basins) from ScienceBase utilizing the SBtools package (Winslow and others, 2015).  

## 

This first section retrieves the stream segment for the entire R10U (5,000+ segments), subsets
them to just those used in Kathy's study.

```{r}
# Mapping Components
# WFS for R10U 
# subset of shapefiles on the project page
# separate call to get subset of features from full WFS
# use HRUs instead of segments or incremental contributing areas
# change line segment width by value
layer<-sbtools::item_get_wfs("571559c2e4b0ef3b7ca864c7")
# need to order the basin names and segs like they are in line 41 sbmaps_segments
segMap<-read.csv("d:/abock/CDI_Mapping/bitbucket/sb_mapping/Streamsegments_Qchange_Buffer.csv",header=T,row.names=1)
segMap<-segMap[-c(186,187),]
# order by POI_ID
segMap<-segMap[with(segMap,order(POI_ID)),]

# subset GF to just the segments we are intereted in
GF_layer<-layer[which(layer@data$POI_ID %in% segMap$POI_ID),]

#GFsegs_buffer<-rgeos::gBuffer(GF_segs,byid=FALSE,width=100,capStyle="ROUND",joinStyle="ROUND")
finalSegs<-sp::spTransform(GF_layer,"+init=epsg:4326")
# This is the reprojection to WGS84 web mercator
#finalSegs<-sp::spTransform(GF_layer,"+init=epsg:3857")
finalSegs<-finalSegs[order(finalSegs$POI_ID),]
# Add the basin name, code, and segment number
# so that they can be used in Shiny
finalSegs@data$Basin<-segMap$Basin
finalSegs@data$Code<-segMap$Code
finalSegs@data$Nseg<-segMap$Nseg
finalSegs<-finalSegs[order(finalSegs$Nseg),]
```

This next set of functions opens and reads each file and then binds them 
together in a single data frame.
```{r}
# baseFlow<-lapply(sbFiles$url[baseLine],RCurl::getURL)
# baseAll<-lapply(baseFlow,function(i){
#   read.csv(text=unlist(i),row.names=1)
# })
# # NOTE for this we need use the .id argument in bind_cols to add
# # the site names to the column names of the DF, do this next week
# baseData<-dplyr::bind_cols(baseAll)
# row.names(baseData)<-rownames(baseAll[[1]])
# print (dim(baseData))
```

```{r}
# futFlow<-lapply(sbFiles$url[future],RCurl::getURL)
# futAll<-lapply(futFlow,function(i){
#   read.csv(text=unlist(i),row.names=1)
# })
# futData<-dplyr::bind_cols(futAll)
# rownames(futData)<-rownames(futAll[[1]])
# print (dim(futData))
```

Calculate the mean for the period (i.e. 2055).  We can probably 
automate to pick the statistic (i.e. Mean, SD, Variability) if we want.

```{r}
# #Means
# baseMeans<-colMeans(baseData)
# futMeans<-colMeans(futData)
# sfMeans<-head(baseMeans-futMeans,-2)
```

## 2B.  Summarizing data for a single site across the entire period of record

The first step is to grab all GCMs for all POR fora single site.  We are 
just going to grab one period of record for now.  Right now this is setup to 
get the mean monthly data for the site.

```{r}
# baseLine_OF<-grep(paste(c("OF","1982","BASELINE"),collapse="_"),sbFiles$fname)
# future_OF<-grep(paste(c("OF","2055"),collapse="_"),sbFiles$fname)
```

This is the same set of functions we used above
```{r}
# # retrieve baseflow data for all sites
# baseOF <- RCurl::getURL(sbFiles$url[baseLine_OF])
# baseOF2 <- read.csv(text=baseOF)
# names(baseOF2)
# 
# futOF<-lapply(sbFiles$url[future_OF],RCurl::getURL)
# futAllOF<-lapply(futOF,function(i){
#   read.csv(text=unlist(i),row.names=1)
# })
# # look to use the .id argument for bind_cols to identity GCM's with their specific columns
# futDataOF<-dplyr::bind_cols(futAllOF)
# futDataOF$date<-rownames(futAllOF[[1]])
```

Here, the data is converted to a zoo object, and the mean monthly values are calculated.
```{r}
# # convert to zoo object for annual and seasonal aggregation if necessary
# baseZoo<-zoo::read.zoo(baseOF2,index.column=1,format="%Y-%m-%d")
# futZoo<-zoo::read.zoo(futDataOF,index.column=length(colnames(futDataOF)),
#                       format="%Y-%m-%d")
# 
# # get the mean monthly data, turn off warnings.
# options(warn=-1)
# FutMM <- aggregate(futDataOF, list(data.table::month(as.Date(futDataOF$date))), 
#                    mean,na.rm=T)
# baseMM <- aggregate(baseOF2, list(data.table::month(as.Date(futDataOF$date))), 
#                     mean, na.rm=T)
# options(warn=0)
```

# 3. Matching Kathy's segments up with the Geospatial Fabric

This section retrieves the segments for the Geospatial Fabric for Region 10u.  
Note this is hosted on one of my SB item pages.  I need to talk to Roland when
he gets back from Sacremento to see if there is an alternative location 
to access this

```{r}
# # WFS for R10U
# layer<-sbtools::item_get_wfs("571559c2e4b0ef3b7ca864c7")
# layer_dd<-sp::spTransform(layer,"+init=epsg:4326")

```

The csv below is the key for matching Kathy's segments up to the GeoFab. Right now
this is accessed locally. Notes: 
1.  We need to remove the two extraneous segments from the mussellshell River Kathy 
    found on Friday
2.  If we are going to buffer the polylines, we need to do so before we project the
    shapefile from albers to geographic.
3.  Right now we are re-projecting to WGS84, we need to determine if it is possible
    to re-project to WGS84 web mercator in R.

```{r}
# # can only buffer shapefiles with projected coordinates?
# #buffer<-rgeos::gBuffer(GF_segs,byid=FALSE,width=3,capStyle="ROUND",joinStyle="ROUND")
# 
# # need to order the basin names and segs like they are in line 41 sbmaps_segments
# segMap<-read.csv("d:/abock/CDI_Mapping/SB_Mapping/PRMSdata/Streamsegments_Qchange_Buffer.csv",header=T,row.names=1)
# # remove the dummy rows, and two rows to get to 185 (fix this next week)
# segIDs<-head(segMap$POI_ID,-4)
# # reproject to wgs84
# GF_segs<-layer_dd[which(layer_dd$POI_ID %in% segIDs),]
# sp::plot(GF_segs,col="red")
```

This next section will take the accessed SB data and GIS features, map them onto a leaflet map, and create several
summary graphs and downloadable files.  Note that the "shiny" app will be built upon this functionality.
Functions are a part of package specificed (*sbtools::item_get*  *item_get* is function in *sbtools* library)

## 1.  Plotting the Data

Import the necessary libraries
```{r}
# library(leaflet)
# library(RColorBrewer)
```

A couple of different ways to present color ramps to symbolize the segments with

```{r}
# #pal<-colorNumeric(
# #  palette = "Blues",
# #  domain=sfMeans
# #)
# 
# pal2 <- colorQuantile("YlGn", NULL, n = 10)
```

This next section determines what information is in the popup balloons when a user clicks on a feature/segment.
This needs to be changed to the stream GNIS name. (what else?)

```{r}
# popup <- paste0("<strong>Name: </strong>", 
#                 GF_segs@data$seg_id)
```

Here is the map!
Note that we can offer an option to change the basemap (I believe)
Things to Do before carrying map over to shiny:
  1.  Add buffer to shapefile
  2.  Customize popup
  3.  Add legend to map
  4.  Set Bounds for map based on feature extent

```{r}
# leaflet(GF_segs) %>% addTiles() %>% addPolylines(color=~pal2(sfMeans),weight=2,popup=~popup)
